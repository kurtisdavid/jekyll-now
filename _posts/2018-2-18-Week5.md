---
layout: post
title: Week 5
---

### What did you do this past week?
This past week was dedicated to IDB1, since our team got a late start. I pushed off all my other deadlines to work on the project, expecting to barely finish in time for the due date. Surprisingly though, we got it all finished with a whole day to spare, so that felt great! Afterwards I just went straight for my other homeworks; I'll continue working on IDB2 sometime next week.

### What's in your way?
For IDB, it's the fact that all of the APIs we decided to use require partnership/development access (as opposed to just a public api). I was honestly worried we wouldn't be able to get any data for IDB1, but sure enough a few companies gave us access this week. I am still waiting for more replies, so that I can get started scraping our data for IDB2. Also, some APIs don't provide all the information we want, so I've had to come up with ways to scrape leftover data from the websites themselves.

### What will you do next week?
I'll probably start writing the code and unit tests to scrape our data and help with transferring them to our AWS database. I'll also be working on Computer Vision, and any other homeworks released so that I can get them done early - midterms are coming up soon. I'm especially worried about Algorithms, since Dr. Ramachandran dished out a lot of topics before the exam.

### What's my experience of the class?
This week was more interesting than usual, I like the increased amount of HackerRanks in class. I also learned a few things about iterators in Python, as well as some JavaScript tricks (like anonymous functions). I was actually a little worried about the presentations on Wednesday, since our website wasn't entirely finished, but luckily we ended up being the team without any customer to present to. Instead, one of the TAs took a look and he seemed to like our website idea! 

### What's my pick-of-the-week or tip-of-the-week?
Convolutional neural networks have been extensively used for computer vision, but have you ever wondered how the network actually applies its learned kernels on the inputs? Well I stumbled onto this [nice visualization](https://ezyang.github.io/convolution-visualizer/index.html) that lets you experiment with different parameters such as input size, kernel size, padding, dilation and stride. This can help a lot when first building your own CNN!